---
permalink: maintain/decommissioning-disconnected-grid-nodes.html 
sidebar: sidebar 
keywords: storagegrid, disconnected nodes, decommission node 
summary: 'Möglicherweise müssen Sie einen Knoten außer Betrieb setzen, der derzeit nicht mit dem Grid \(One verbunden ist, dessen Status unbekannt oder administrativ ausgefallen ist\).' 
---
= Die getrennten Grid-Nodes werden deaktiviert
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
Möglicherweise müssen Sie einen Knoten außer Betrieb setzen, der derzeit nicht mit dem Grid verbunden ist (einen Node, dessen Status unbekannt oder administrativ ausgefallen ist).

.Bevor Sie beginnen
* Sie verstehen die Überlegungen zur Stilllegung link:considerations-for-decommissioning-admin-or-gateway-nodes.html["Admin- und Gateway-Nodes"] und die Überlegungen zur Stilllegung link:considerations-for-decommissioning-storage-nodes.html["Storage-Nodes"].
* Sie haben alle erforderlichen Elemente erhalten.
* Sie haben sichergestellt, dass keine Datenreparaturjobs aktiv sind. Siehe link:checking-data-repair-jobs.html["Prüfen Sie die Reparatur von Daten"].
* Sie haben bestätigt, dass die Wiederherstellung von Storage-Nodes an keiner Stelle im Grid ausgeführt wird. In diesem Fall müssen Sie warten, bis alle Cassandra-Rebuilds im Rahmen der Recovery abgeschlossen sind. Anschließend können Sie mit der Stilllegung fortfahren.
* Sie haben sichergestellt, dass andere Wartungsvorgänge während der Deaktivierung des Nodes nicht ausgeführt werden, es sei denn, der Vorgang zur Deaktivierung des Nodes wurde angehalten.
* Die Spalte *Decommission möglich* für den Knoten oder Knoten, die Sie außer Betrieb nehmen möchten, enthält ein grünes Häkchen.
* Sie haben die Provisionierungs-Passphrase.


.Über diese Aufgabe
Sie können getrennte Knoten identifizieren, indem Sie in der Spalte *Health* nach dem blauen Symbol Unbekannt oder dem grauen Symbol administrativ unten image:../media/icon_alarm_gray_administratively_down.png["Symbol administrativ nicht verfügbar"]suchenimage:../media/icon_alarm_blue_unknown.png["Symbol Unbekannt"].

Beachten Sie vor dem Stilllegen getrennter Nodes Folgendes:

* Dieses Verfahren dient in erster Linie zum Entfernen eines einzelnen nicht verbundenen Knotens. Wenn Ihr Grid mehrere getrennte Knoten enthält, muss die Software gleichzeitig ausmustern, wodurch das Potenzial für unerwartete Ergebnisse erhöht wird.
+

CAUTION: Es kann zu Datenverlusten kommen, wenn Sie mehr als einen getrennten Storage Node gleichzeitig stilllegen. Siehe link:considerations-for-decommissioning-storage-nodes.html#considerations-disconnected-storage-nodes["Überlegungen zu getrennten Storage-Nodes"].

+

CAUTION: Gehen Sie mit Vorsicht vor, wenn Sie Storage-Nodes in einem Grid stilllegen, das rein softwarebasierte Metadaten-Nodes enthält. Wenn Sie alle Knoten außer Betrieb nehmen, die für den Speicher _sowohl_ Objekte als auch Metadaten konfiguriert sind, wird die Fähigkeit zum Speichern von Objekten aus dem Raster entfernt. Weitere Informationen zu nur Metadaten-Storage-Nodes finden Sie unterlink:../primer/what-storage-node-is.html#types-of-storage-nodes["Typen von Storage-Nodes"].

* Wenn ein getrennter Knoten nicht entfernt werden kann (z. B. ein Speicher-Knoten, der für das ADC-Quorum erforderlich ist), kann kein anderer getrennter Knoten entfernt werden.


.Schritte
. Versuchen Sie, alle nicht verbundenen Grid-Nodes wieder online zu schalten oder wiederherzustellen, sofern Sie einen Archive Node nicht stilllegen (der getrennt werden muss).
+
Anweisungen finden Sie unter link:warnings-and-considerations-for-grid-node-recovery.html["Verfahren zur Recovery von Grid-Nodes"] .

. Wenn Sie einen nicht verbundenen Grid-Node nicht wiederherstellen können und ihn während der Trennung außer Betrieb nehmen möchten, aktivieren Sie das Kontrollkästchen für diesen Node.
+

NOTE: Wenn Ihr Grid mehrere getrennte Knoten enthält, muss die Software gleichzeitig ausmustern, wodurch das Potenzial für unerwartete Ergebnisse erhöht wird.

+

CAUTION: Seien Sie vorsichtig, wenn Sie mehrere getrennte Grid-Nodes gleichzeitig stilllegen möchten, insbesondere wenn Sie mehrere getrennte Storage-Nodes auswählen. Wenn Sie mehr als einen getrennten Storage Node haben, den Sie nicht wiederherstellen können, wenden Sie sich an den technischen Support, um die beste Vorgehensweise zu ermitteln.

. Geben Sie die Provisionierungs-Passphrase ein.
+
Die Schaltfläche *Start Decommission* ist aktiviert.

. Klicken Sie Auf *Start Decommission*.
+
Es wird eine Warnung angezeigt, die angibt, dass Sie einen nicht verbundenen Knoten ausgewählt haben und dass Objektdaten verloren gehen, wenn der Knoten die einzige Kopie eines Objekts hat.

. Überprüfen Sie die Liste der Knoten, und klicken Sie auf *OK*.
+
Der Außerbetriebnahmevorgang wird gestartet und der Fortschritt wird für jeden Knoten angezeigt.  Während des Vorgangs wird ein neues Wiederherstellungspaket generiert, das die Änderung der Netzkonfiguration enthält.

. Sobald das neue Wiederherstellungspaket verfügbar ist, klicken Sie auf den Link oder wählen Sie *Wartung* > *System* > *Wiederherstellungspaket*, um auf die Seite mit dem Wiederherstellungspaket zuzugreifen.  Laden Sie dann die `.zip` Datei.
+
Siehe die Anweisungen fürlink:downloading-recovery-package.html["Herunterladen des Wiederherstellungspakets"] .

+

NOTE: Laden Sie das Wiederherstellungspaket so schnell wie möglich herunter, um sicherzustellen, dass Sie Ihr Netz wiederherstellen können, falls während der Außerbetriebnahme etwas schiefgeht.

+

CAUTION: Die Wiederherstellungspaketdatei muss gesichert werden, da sie Verschlüsselungsschlüssel und Passwörter enthält, mit denen Daten aus dem StorageGRID -System abgerufen werden können.

. Überwachen Sie die Seite Dekommission regelmäßig, um sicherzustellen, dass alle ausgewählten Knoten erfolgreich außer Betrieb gesetzt werden.
+
Storage-Nodes können Tage oder Wochen ausmustern. Wenn alle Aufgaben abgeschlossen sind, wird die Liste der Knotenauswahl mit einer Erfolgsmeldung erneut angezeigt. Wenn Sie einen getrennten Speicherknoten außer Betrieb genommen haben, zeigt eine Informationsmeldung an, dass die Reparaturaufträge gestartet wurden.

. Nachdem die Nodes im Rahmen der Stilllegung automatisch heruntergefahren wurden, entfernen Sie alle verbleibenden Virtual Machines oder anderen Ressourcen, die dem ausgemusterten Node zugeordnet sind.
+

CAUTION: Führen Sie diesen Schritt erst aus, wenn die Nodes automatisch heruntergefahren wurden.

. Wenn Sie einen Storage Node außer Betrieb nehmen, überwachen Sie den Status der Reparatur-Jobs mit *replizierten Daten* und *Erasure-codierten (EC) Daten*, die während des Stilllegungsprozesses automatisch gestartet werden.


[role="tabbed-block"]
====
.Replizierte Daten
--
* Um einen geschätzten Fertigstellungsgrad für die replizierte Reparatur zu erhalten, fügen Sie die Option zum Befehl Repair-Data hinzu `show-replicated-repair-status`.
+
`repair-data show-replicated-repair-status`

* So stellen Sie fest, ob Reparaturen abgeschlossen sind:
+
.. Wählen Sie *Knoten* > *_Speicherknoten wird repariert_* > *ILM*.
.. Prüfen Sie die Attribute im Abschnitt Bewertung. Wenn die Reparaturen abgeschlossen sind, weist das Attribut *wartet - Alle* 0 Objekte an.


* So überwachen Sie die Reparatur genauer:
+
.. Wählen Sie *Knoten* aus.
.. Wählen Sie *_Grid Name_* > *ILM* aus.
.. Positionieren Sie den Cursor über dem ILM-Warteschlangendiagramm, um den Wert des Attributs *Scanrate (Objekte/Sek.)* anzuzeigen. Dies ist die Rate, mit der Objekte im Raster gescannt und für ILM in die Warteschlange gestellt werden.
.. Sehen Sie sich im Abschnitt „ILM-Warteschlange“ die folgenden Attribute an:
+
*** *Scan-Zeitraum - geschätzt*: Die geschätzte Zeit, um einen vollständigen ILM-Scan aller Objekte durchzuführen.
+
Ein vollständiger Scan garantiert nicht, dass ILM auf alle Objekte angewendet wurde.

*** *Reparaturversuche*: Die Gesamtzahl der versuchten Objektreparaturvorgänge für replizierte Daten, die als hohes Risiko gelten.  Objekte mit hohem Risiko sind alle Objekte, von denen eine Kopie übrig bleibt, unabhängig davon, ob dies durch die ILM-Richtlinie festgelegt ist oder aufgrund verlorener Kopien.  Dieser Zähler erhöht sich jedes Mal, wenn ein Speicherknoten versucht, ein Hochrisikoobjekt zu reparieren.  Bei einer Netzüberlastung werden ILM-Reparaturen mit hohem Risiko priorisiert.
+
Die gleiche Objektreparatur kann erneut inkrementiert werden, wenn die Replikation nach der Reparatur fehlgeschlagen ist.  + Diese Attribute können nützlich sein, wenn Sie den Fortschritt der Wiederherstellung des Storage Node-Volumes überwachen.  Wenn die Anzahl der Reparaturversuche nicht mehr zunimmt und ein vollständiger Scan abgeschlossen wurde, ist die Reparatur wahrscheinlich abgeschlossen.



.. Alternativ senden Sie eine Prometheus-Abfrage für `storagegrid_ilm_scan_period_estimated_minutes` Und `storagegrid_ilm_repairs_attempted` .




--
.EC-Daten (Erasure Coded)
--
So überwachen Sie die Reparatur von Daten mit Verfahren zur Einhaltung von Datenkonsistenz und versuchen Sie es erneut, eventuell fehlgeschlagene Anfragen zu senden:

. Status von Datenreparaturen mit Löschungscode ermitteln:
+
** Wählen Sie *Support* > *Tools* > *Metriken*, um die geschätzte Zeit bis zur Fertigstellung und den Fertigstellungsgrad für den aktuellen Auftrag anzuzeigen.  Wählen Sie dann im Abschnitt „Grafana“ die Option „EC-Übersicht“ aus.  Sehen Sie sich die Dashboards *Geschätzte Zeit bis zur Fertigstellung des Grid EC-Jobs* und *Prozentsatz der Fertigstellung des Grid EC-Jobs* an.
** Mit diesem Befehl können Sie den Status einer bestimmten Operation anzeigen `repair-data`:
+
`repair-data show-ec-repair-status --repair-id repair ID`

** Verwenden Sie diesen Befehl, um alle Reparaturen aufzulisten:
+
`repair-data show-ec-repair-status`

+
Die Ausgabe listet Informationen auf, einschließlich `repair ID`, für alle zuvor ausgeführten und aktuell laufenden Reparaturen.



. Wenn die Ausgabe zeigt, dass der Reparaturvorgang fehlgeschlagen ist, verwenden Sie `--repair-id` die Option, um die Reparatur erneut zu versuchen.
+
Mit diesem Befehl wird eine fehlerhafte Node-Reparatur mithilfe der Reparatur-ID 6949309319275667690 erneut versucht:

+
`repair-data start-ec-node-repair --repair-id 6949309319275667690`

+
Mit diesem Befehl wird eine fehlerhafte Volume-Reparatur mithilfe der Reparatur-ID 6949309319275667690 wiederholt:

+
`repair-data start-ec-volume-repair --repair-id 6949309319275667690`



--
====
.Nachdem Sie fertig sind
Sobald die getrennten Nodes außer Betrieb genommen und alle Reparatur-Jobs abgeschlossen sind, können Sie alle verbundenen Grid-Nodes je nach Bedarf ausmustern.

Führen Sie anschließend die folgenden Schritte aus, nachdem Sie den Vorgang zur Deaktivierung abgeschlossen haben:

* Stellen Sie sicher, dass die Laufwerke des ausgemusterten Grid-Node sauber gelöscht werden. Verwenden Sie ein handelsübliches Datenwischwerkzeug oder einen Dienst, um die Daten dauerhaft und sicher von den Laufwerken zu entfernen.
* Wenn Sie einen Appliance-Node deaktiviert haben und die Daten auf der Appliance mithilfe der Node-Verschlüsselung geschützt wurden, löschen Sie die Konfiguration des Verschlüsselungsmanagement-Servers (Clear KMS) mithilfe des StorageGRID Appliance Installer. Wenn Sie die Appliance einem anderen Grid hinzufügen möchten, müssen Sie die KMS-Konfiguration löschen. Anweisungen hierzu finden Sie unter https://docs.netapp.com/us-en/storagegrid-appliances/commonhardware/monitoring-node-encryption-in-maintenance-mode.html["Überwachung der Node-Verschlüsselung im Wartungsmodus"^].

